{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "w/o bert toknizer 버전 \n",
    "기존의 문장에서의 split 만을 사용하여 \n",
    "BLEU-4 Score를 측정하였음, \n",
    "\n",
    "기존의 ARPER 논문의 BLEU-4 score를 정의하는게 \n",
    "제대로 설명이 안되어있는데 \n",
    "통상적으로 BLEU-4 Score는 4-GRAM짜리의 BLEU스코어를\n",
    "의미하기도 하지만 \n",
    "cumulative 1~4개 까지의 평균도 BLEU-4 라고 불림 \n",
    "\n",
    "그래서 일단 두개의 코드를 다 적어둠\n",
    "\"\"\"\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 설정 구간 \n",
    "\n",
    "#test.sh의 환경변수 그대로 \n",
    "DATA_DIR= 'multiwoz2_data'\n",
    "DOMAIN= '0_attraction'\n",
    "dataset_path = \"{}/{}\".format(DATA_DIR, DOMAIN)\n",
    "model_checkpoint = \"./runs/latest/\"\n",
    "\n",
    "# Output file 파일명적기 \n",
    "output_file = 'Oct08_03-53-49_output.txt'\n",
    "output_file_path = model_checkpoint+output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset load\n",
    "with open(\"{}/{}_test.txt\".format(DATA_DIR, DOMAIN), 'r') as f:\n",
    "    golden_responses =  [] \n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        _, a = line.split('|', maxsplit=1)\n",
    "        golden_responses.append(a.strip().split())\n",
    "\n",
    "# predict dataset load\n",
    "with open(output_file_path, 'r') as f:\n",
    "    predict_responses = []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        predict_responses.append(line.strip().split())    \n",
    "\n",
    "assert len(golden_responses) == len(predict_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DEBUG\n",
    "# 실행안해두됨\n",
    "\"\"\"\n",
    "https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L425\n",
    "\n",
    "class SmoothingFunction:\n",
    "This is an implementation of the smoothing techniques\n",
    "for segment-level BLEU scores that was presented in\n",
    "Boxing Chen and Collin Cherry (2014) A Systematic Comparison of\n",
    "Smoothing Techniques for Sentence-Level BLEU. In WMT14.\n",
    "http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
    "\"\"\"\n",
    "for golden, predict in zip(golden_responses, predict_responses):\n",
    "    print(\"G : {}\".format(golden))\n",
    "    print(\"P : {}\".format(predict))\n",
    "    print('Individual 1-gram: %f' % sentence_bleu(golden, predict, weights=(1, 0, 0, 0)))\n",
    "    print('Individual 2-gram: %f' % sentence_bleu(golden, predict, weights=(0, 1, 0, 0), smoothing_function=SmoothingFunction().method4))\n",
    "    print('Individual 3-gram: %f' % sentence_bleu(golden, predict, weights=(0, 0, 1, 0), smoothing_function=SmoothingFunction().method4))\n",
    "    print('Individual 4-gram: %f' % sentence_bleu(golden, predict, weights=(0, 0, 0, 1), smoothing_function=SmoothingFunction().method4))\n",
    "    print('cumulative 4-gram: %f' % sentence_bleu(golden, predict, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score 계산부분\n",
    "BLEU_scores = [0, 0, 0, 0, 0]\n",
    "\n",
    "for golden, predict in zip(golden_responses, predict_responses):\n",
    "    blue1 = sentence_bleu(golden, predict, weights=(1, 0, 0, 0))\n",
    "    blue2 = sentence_bleu(golden, predict, weights=(0, 1, 0, 0), smoothing_function=SmoothingFunction().method4)\n",
    "    blue3 = sentence_bleu(golden, predict, weights=(0, 0, 1, 0), smoothing_function=SmoothingFunction().method4)\n",
    "    blue4 = sentence_bleu(golden, predict, weights=(0, 0, 0, 1), smoothing_function=SmoothingFunction().method4)\n",
    "    blue4_cum = sentence_bleu(golden, predict, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method4)\n",
    "    \n",
    "    BLEU_scores[0] += blue1\n",
    "    BLEU_scores[1] += blue2\n",
    "    BLEU_scores[2] += blue3\n",
    "    BLEU_scores[3] += blue4\n",
    "    BLEU_scores[4] += blue4_cum\n",
    "\n",
    "print('Individual 1-gram: ', BLEU_scores[0] / len(predict_responses))\n",
    "print('Individual 2-gram: ', BLEU_scores[1] / len(predict_responses))\n",
    "print('Individual 3-gram: ', BLEU_scores[2] / len(predict_responses))\n",
    "print('Individual 4-gram: ', BLEU_scores[3] / len(predict_responses))\n",
    "print('cumulative 4-gram: ', BLEU_scores[4] / len(predict_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
