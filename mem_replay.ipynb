{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Transformer >= 3.0.0 이상부터\n",
    "# BertTokenizer.toknizer()사용가능 \n",
    "\n",
    "# vizualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# pytorch library\n",
    "import torch # the main pytorch library\n",
    "import torch.nn.functional as f # the sub-library containing different functions for manipulating with tensors\n",
    "\n",
    "# huggingface's transformers library\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_version = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_version)\n",
    "model = BertModel.from_pretrained(bert_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(distances, figsize=(20, 10), titles=None):\n",
    "    # get the number of columns\n",
    "    ncols = len(distances)\n",
    "    # create the subplot placeholders\n",
    "    fig, ax = plt.subplots(ncols=ncols, figsize=figsize)\n",
    "    \n",
    "    for i in range(ncols):\n",
    "        \n",
    "        # get the axis in which we will draw the matrix\n",
    "        axes = ax[i] if ncols > 1 else ax\n",
    "        \n",
    "        # get the i-th distance\n",
    "        distance = distances[i]\n",
    "        \n",
    "        # create the heatmap\n",
    "        axes.imshow(distance)\n",
    "        \n",
    "        # show the ticks\n",
    "        axes.set_xticks(np.arange(distance.shape[0]))\n",
    "        axes.set_yticks(np.arange(distance.shape[1]))\n",
    "        \n",
    "        # set the tick labels\n",
    "        axes.set_xticklabels(np.arange(distance.shape[0]))\n",
    "        axes.set_yticklabels(np.arange(distance.shape[1]))\n",
    "        \n",
    "        # set the values in the heatmap\n",
    "        for j in range(distance.shape[0]):\n",
    "            for k in range(distance.shape[1]):\n",
    "                text = axes.text(k, j, str(round(distance[j, k], 3)),\n",
    "                               ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "        # set the title of the subplot\n",
    "        title = titles[i] if titles and len(titles) > i else \"Text Distance\"\n",
    "        axes.set_title(title, fontsize=\"x-large\")\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def load_data(_path):\n",
    "    datas = [] \n",
    "    with open(_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:10]:\n",
    "            datas.append(line.strip())\n",
    "    return datas\n",
    "\n",
    "def load_data_at_EM(memory):\n",
    "    datas = [] \n",
    "    for tmp in memory:\n",
    "        datas.append(tmp['text'])\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 0_attraction domain 10 lines loads\n",
      ">>> 1_booking domain 10 lines loads\n",
      ">>> 2_hotel domain 10 lines loads\n",
      ">>> 3_restaurant domain 10 lines loads\n",
      ">>> 4_taxi domain 10 lines loads\n",
      ">>> 5_train domain 10 lines loads\n"
     ]
    }
   ],
   "source": [
    "##### CONFIG AREA\n",
    "THRESHOLD = 0.98 # 0.85\n",
    "THRESHOLD_F_TO_LONGTERM = 2 # F: ? 이상은 Longterm으로 전이시킬 후보\n",
    "SHORTTERM_REPLAY_RATIO = 0.2 # 현재도메인의 20%개수만을 가져옴 \n",
    "data_template = {\n",
    "    'T' : 0,  # 들어오고나서 지난횟수 \n",
    "    'F' : 0,  # cos simil의 max값이 > Threshold한 횟수 \n",
    "    'D' : '', # Domain label\n",
    "    'text' : ''\n",
    "}\n",
    "\n",
    "##### MEM retention var\n",
    "freq_0_retention_timestep = 1 # F:0의 데이터들은 T:1까지 살려둠(1회반복)\n",
    "freq_1_retention_timestep = 3 # F:1의 데이터들은 T:3까지 살려둠(3회반복)\n",
    "\n",
    "##### Domain data loader \n",
    "DATA_DIR = './multiwoz2_data/1008_multiline_process/'\n",
    "# ARPER RUM1: 0 5 2 1 3 4 \n",
    "DOMAIN_SEQ = ['0_attraction', '5_train', '2_hotel', '1_booking', '3_restaurant', '4_taxi', ]\n",
    "data_dict = {}\n",
    "\n",
    "for domain in DOMAIN_SEQ:\n",
    "    datas = load_data(DATA_DIR+\"{}_train.txt\".format(domain,))\n",
    "    data_dict[domain] = datas\n",
    "    print(\">>> {} domain {} lines loads\".format(domain, len(datas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Make [0_attraction] domain train data\n",
      "\t Now Episodic Mem : 0 \n",
      ">>> Make [1_booking] domain train data\n",
      "\t Now Episodic Mem : 10 \n",
      "\t Current domain data :  10\n",
      "\t Long term으로 전이될 수 :  0\n",
      "\t EM에서 제거 된 수 :  0\n",
      "\t Replay할 데이터 총 수 :  2\n",
      "\t 도메인당 Replay 데이터 수 :  1\n",
      "\t 실제 Replay 준비된 데이터 수 :  1\n",
      ">>> Make [2_hotel] domain train data\n",
      "\t Now Episodic Mem : 20 \n",
      "\t Current domain data :  10\n",
      "\t Long term으로 전이될 수 :  4\n",
      "\t EM에서 제거 된 수 :  3\n",
      "\t Replay할 데이터 총 수 :  2\n",
      "\t 도메인당 Replay 데이터 수 :  1\n",
      "\t 실제 Replay 준비된 데이터 수 :  2\n",
      ">>> Make [3_restaurant] domain train data\n",
      "\t Now Episodic Mem : 23 \n",
      "\t Current domain data :  10\n",
      "\t Long term으로 전이될 수 :  4\n",
      "\t EM에서 제거 된 수 :  3\n",
      "\t Replay할 데이터 총 수 :  2\n",
      "\t 도메인당 Replay 데이터 수 :  1\n",
      "\t 실제 Replay 준비된 데이터 수 :  2\n",
      ">>> Make [4_taxi] domain train data\n",
      "\t Now Episodic Mem : 26 \n",
      "\t Current domain data :  10\n",
      "\t Long term으로 전이될 수 :  2\n",
      "\t EM에서 제거 된 수 :  7\n",
      "\t Replay할 데이터 총 수 :  2\n",
      "\t 도메인당 Replay 데이터 수 :  1\n",
      "\t 실제 Replay 준비된 데이터 수 :  3\n",
      ">>> Make [5_train] domain train data\n",
      "\t Now Episodic Mem : 27 \n",
      "\t Current domain data :  10\n",
      "\t Long term으로 전이될 수 :  9\n",
      "\t EM에서 제거 된 수 :  4\n",
      "\t Replay할 데이터 총 수 :  2\n",
      "\t 도메인당 Replay 데이터 수 :  1\n",
      "\t 실제 Replay 준비된 데이터 수 :  3\n",
      ">>> DOMAIN SEQ ['0_attraction', '1_booking', '2_hotel', '3_restaurant', '4_taxi', '5_train']\n",
      "\t 0_attraction domain... 10 data\n",
      "\t 1_booking domain... 11 data\n",
      "\t 2_hotel domain... 12 data\n",
      "\t 3_restaurant domain... 12 data\n",
      "\t 4_taxi domain... 13 data\n",
      "\t 5_train domain... 13 data\n",
      ">>> 저장완료\n"
     ]
    }
   ],
   "source": [
    "##### Make train data pool\n",
    "train_data_pool = {}  # {'domain' : [Q|A, Q|A]} 형태 \n",
    "episodic_memory = []  # {'[{template}, {template}]} 형태\n",
    "long_term_pool = [] # Long term에 넘겨질 Pool [Q|A, Q|A] 형태\n",
    "\n",
    "for domain_idx, domain in enumerate(DOMAIN_SEQ):\n",
    "    print(\">>> Make [{}] domain train data\".format(domain))\n",
    "    total_EM_data_num = len(episodic_memory)\n",
    "    print(\"\\t Now Episodic Mem : {} \".format(total_EM_data_num))\n",
    "    \n",
    "    \n",
    "    if episodic_memory == []:# 처음 \n",
    "        train_data_pool[domain] = data_dict[domain].copy() # 처음 도메인 그대로 사용\n",
    "        # EM에 데이터 저장\n",
    "        for data in data_dict[domain]:\n",
    "            episodic_memory.append({\n",
    "                'T' : 0,\n",
    "                'F' : 0,\n",
    "                'D' : domain,\n",
    "                'text' : data\n",
    "            })\n",
    "    else: # EM에 데이터가 쌓여있는 단계 \n",
    "        \"\"\" TODO\n",
    "        1. EM안의 데이터 <-> 새롭게 들어오는데이터 간의\n",
    "           cosine sim값을 계산하여 > Threshold값의 \n",
    "           EM안의 데이터들은 F를 +1해준다\n",
    "        \n",
    "        2. EM policy에 따라 longterm에 넘길꺼, 삭제할거,\n",
    "           train_data_pool에 다시 줄거를 구분 \n",
    "        \n",
    "        3. EM의 Timestep + 1 \n",
    "        \n",
    "        4. 현재도메인데이터 EM에 추가\n",
    "        \n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        ## TODO1 : EM이랑 새로운도메인 COSINE SIMIL값 계산 \n",
    "        prev_data_idx = total_EM_data_num\n",
    "    \n",
    "        current_domain_data = data_dict[domain].copy() # [Q|A, Q|A]\n",
    "        print(\"\\t Current domain data : \", len(data_dict[domain]))\n",
    "        memory_data = load_data_at_EM(episodic_memory).copy()\n",
    "        assert total_EM_data_num==len(memory_data)\n",
    "        #print(memory_data)\n",
    "        \n",
    "        # 기존 + 현재들어오는걸 합쳐서 토크나이징 \n",
    "        texts = memory_data + current_domain_data\n",
    "        \n",
    "        encodings = tokenizer(\n",
    "            texts, # the texts to be tokenized\n",
    "            padding=True, # pad the texts to the maximum length (so that all outputs have the same length)\n",
    "            return_tensors='pt' # return the tensors (not lists)\n",
    "        )\n",
    "        encodings = encodings.to(device)\n",
    "        \n",
    "        # disable gradient calculations\n",
    "        # 임베딩 실시\n",
    "        with torch.no_grad():\n",
    "            embeds = model(**encodings)\n",
    "        embeds = embeds[0]\n",
    "        \n",
    "        # CLS 토큰들 가지고 Cosin simil 계산\n",
    "        CLSs = embeds[:, 0, :]\n",
    "        \n",
    "        # normalize the CLS token embeddings\n",
    "        normalized = f.normalize(CLSs, p=2, dim=1)\n",
    "        # calculate the cosine similarity\n",
    "        cls_dist = normalized.matmul(normalized.T)\n",
    "        cls_dist = cls_dist.numpy()\n",
    "        \n",
    "        #visualize([cls_dist], titles=[\"CLS\"])\n",
    "        em_max_cos_simil = [0 for _ in range(total_EM_data_num) ] \n",
    "        for row in range(total_EM_data_num):\n",
    "            em_max_cos_simil[row] = np.max(cls_dist[row][total_EM_data_num:])\n",
    "            \n",
    "        # TRESHOLD가 넘은거 찾아서 F ++ 해준다 \n",
    "        #print(em_max_cos_simil)\n",
    "        for idx, simil in enumerate(em_max_cos_simil):\n",
    "            if simil > THRESHOLD:\n",
    "                target_data = memory_data[idx]\n",
    "                for tmp in episodic_memory:\n",
    "                    if tmp['text'].strip() == target_data.strip():\n",
    "                        tmp['F'] += 1 \n",
    "        \n",
    "        ## TODO2 : EM 정책에 따라 longterm, 삭제할거, train_pool에 줄거 분류\n",
    "        # Longterm에 넘겨질거 \n",
    "        update_episodic_memory = [] \n",
    "        for data in episodic_memory:\n",
    "            if data['F'] >= 2:\n",
    "                pass # Long term 메모리로 보내서 작업할 데이터들 \n",
    "            else:\n",
    "                update_episodic_memory.append(data)\n",
    "        print(\"\\t Long term으로 전이될 수 : \", len(episodic_memory)-len(update_episodic_memory))\n",
    "        episodic_memory = update_episodic_memory.copy() # update \n",
    "        \n",
    "        # 삭제할거 \n",
    "        #freq_0_retention_timestep = 1 # F:0의 데이터들은 T:1까지 살려둠\n",
    "        #freq_1_retention_timestep = 3 # F:1의 데이터들은 T:3까지 살려둠\n",
    "        update_episodic_memory = []\n",
    "        for data in episodic_memory:\n",
    "            if data['F'] == 0:\n",
    "                if data['T'] >= freq_0_retention_timestep:\n",
    "                    pass # 삭제할거\n",
    "                else:\n",
    "                    update_episodic_memory.append(data)\n",
    "                    \n",
    "            elif data['F'] == 1:\n",
    "                if data['T'] >= freq_1_retention_timestep:\n",
    "                    pass # 삭제할거 \n",
    "                else:\n",
    "                    update_episodic_memory.append(data)\n",
    "        \n",
    "        print(\"\\t EM에서 제거 된 수 : \", len(episodic_memory)-len(update_episodic_memory))\n",
    "        episodic_memory = update_episodic_memory.copy() # update\n",
    "        \n",
    "        # train_pool에 현재도메인 + EM메모리 추가 \n",
    "        # !! 여기서 개수를 조정할수도 있음 현재는 20퍼\n",
    "        # 넘겨주는건 각도메인마다 현재데이터*20퍼 / 도메인개수 \n",
    "        replay_data_list = [] \n",
    "        \n",
    "        replay_data_num = math.ceil(len(current_domain_data) * SHORTTERM_REPLAY_RATIO)\n",
    "        per_domain_replay_num = math.ceil(replay_data_num / (domain_idx + 1))\n",
    "        print(\"\\t Replay할 데이터 총 수 : \", replay_data_num)\n",
    "        print(\"\\t 도메인당 Replay 데이터 수 : \", per_domain_replay_num)\n",
    "        \n",
    "        # 도메인별로 정리 \n",
    "        domain_data_dict = {}\n",
    "        for data in episodic_memory:\n",
    "            try:\n",
    "                domain_data_dict[data['D']].append(data['text'])\n",
    "            except:\n",
    "                domain_data_dict[data['D']] = [data['text']]\n",
    "        \n",
    "        # 도메인별 돌면서 뽑기 \n",
    "        for dom_k, dom_v in domain_data_dict.items():\n",
    "            replay_data_list += random.sample(dom_v, per_domain_replay_num)\n",
    "        \n",
    "        print(\"\\t 실제 Replay 준비된 데이터 수 : \", len(replay_data_list))\n",
    "        \n",
    "        \n",
    "        train_data_pool[domain] = current_domain_data.copy()\n",
    "        for replay_data in replay_data_list:\n",
    "            train_data_pool[domain].append(replay_data)\n",
    "            \n",
    "        ## TODO3: Timestep + 1 \n",
    "        for data in episodic_memory:\n",
    "            data['T'] += 1 \n",
    "        \n",
    "        ## TODO4: 현재도메인 데이터 EM에 추가\n",
    "        for data in data_dict[domain]:\n",
    "            episodic_memory.append({\n",
    "                'T' : 0,\n",
    "                'F' : 0,\n",
    "                'D' : domain,\n",
    "                'text' : data\n",
    "            })\n",
    "        \n",
    "print(\">>> DOMAIN SEQ\", DOMAIN_SEQ)\n",
    "for dom_k in DOMAIN_SEQ:\n",
    "    print(\"\\t {} domain... {} data\".format(dom_k, len(train_data_pool[dom_k])))\n",
    "\n",
    "#print(train_data_pool)\n",
    "#print(len(train_data_pool['0_attraction']))\n",
    "#print(len(train_data_pool['1_booking']))\n",
    "#print(len(train_data_pool['2_hotel']))\n",
    "\n",
    "\n",
    "# 끝으로 원하는 Domain_seq별 데이터들 저장해주기 \n",
    "folder_name = \"./multiwoz2_data/\"\n",
    "for domain in DOMAIN_SEQ:\n",
    "    folder_name += domain[0]\n",
    "folder_name = folder_name + \"_data/\"    \n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for dom_k, dom_v in train_data_pool.items():\n",
    "    with open(folder_name + \"{}_train.txt\".format(dom_k), 'w') as file:\n",
    "        for data in dom_v:\n",
    "            file.write(\"{}\\n\".format(data))\n",
    "print(\">>> 저장완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
